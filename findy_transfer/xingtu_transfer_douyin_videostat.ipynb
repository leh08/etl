{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "from datetime import datetime, timezone\n",
    "from google.cloud import storage\n",
    "import gcsfs\n",
    "import re\n",
    "from sqlalchemy import create_engine, text\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_config = {\n",
    "    \"host\": \"findy-medium-stage.czmgcqkw4ett.ap-southeast-1.rds.amazonaws.com\",\n",
    "    \"database\": \"findy_medium_stage\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"F!nDy!Med!umStage2o24\",\n",
    "    \"port\": \"5432\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_upsert_query(cols: List[str],\n",
    "                       table_name: str,\n",
    "                       unique_key: List[str]=[],\n",
    "                       cols_not_for_update: List[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Builds postgres upsert query using input arguments.\n",
    "    Note: In the absence of unique_key, this will be just an insert query.\n",
    "    Example : build_upsert_query(\n",
    "        ['col1', 'col2', 'col3', 'col4'],\n",
    "        \"my_table\",\n",
    "        ['col1'],\n",
    "        ['col2']\n",
    "    ) ->\n",
    "    INSERT INTO my_table (col1, col2, col3, col4) VALUES %s\n",
    "    ON CONFLICT (col1) DO UPDATE SET (col3, col4) = (EXCLUDED.col3, EXCLUDED.col4) ;\n",
    "    :param cols: the postgres table columns required in the\n",
    "        insert part of the query.\n",
    "    :param table_name: the postgres table name.\n",
    "    :param unique_key: unique_key of the postgres table for checking\n",
    "        unique constraint violations.\n",
    "    :param cols_not_for_update: columns in cols which are not required in\n",
    "        the update part of upsert query.\n",
    "    :return: Upsert query as per input arguments.\n",
    "    \"\"\"\n",
    "    cols = [f'\"{col}\"' for col in cols]\n",
    "    cols_str = ', '.join(cols)\n",
    "    insert_query = \"\"\" INSERT INTO %s (%s) VALUES %%s \"\"\" % (\n",
    "        table_name, cols_str\n",
    "    )\n",
    "    if cols_not_for_update is not None:\n",
    "        cols_not_for_update.extend(unique_key)\n",
    "    else:\n",
    "        cols_not_for_update = [col for col in unique_key]\n",
    "    cols_not_for_update = [f'\"{col}\"' for col in cols_not_for_update]\n",
    "    unique_key = [f'\"{col}\"' for col in unique_key]\n",
    "    unique_key_str = ', '.join(unique_key)\n",
    "\n",
    "    update_cols = [f\"{col}\" for col in cols if col not in cols_not_for_update]\n",
    "    update_cols_str = ', '.join(update_cols)\n",
    "    update_cols_with_excluded_markers = [f'EXCLUDED.{col}' for col in update_cols]\n",
    "    update_cols_with_excluded_markers_str = ', '.join(\n",
    "        update_cols_with_excluded_markers\n",
    "    )\n",
    "    if len(update_cols) > 1:\n",
    "        equality_clause = \"(%s) = (%s)\"\n",
    "    else:\n",
    "        equality_clause = \"%s = %s\"\n",
    "\n",
    "    on_conflict_clause = f\"\"\" ON CONFLICT (%s) DO UPDATE SET {equality_clause} ;\"\"\"\n",
    "    on_conflict_clause = on_conflict_clause % (\n",
    "        unique_key_str,\n",
    "        update_cols_str,\n",
    "        update_cols_with_excluded_markers_str\n",
    "    )\n",
    "    if len(unique_key) == 0:\n",
    "        return insert_query\n",
    "    return insert_query + on_conflict_clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client(project=\"ytone-430507\")\n",
    "today = datetime.now()\n",
    "table_name = \"douyin_videostat\"\n",
    "api_name = \"author_audience_distribution\"\n",
    "bucket_name = \"4_data_warehouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'blob': <Blob: 4_data_warehouse, 1_xingtu/douyin_videostat/2024-09-08/douyin_videostat_240918_0.parquet, 1726643320144311>,\n",
       "  'date': '2024-09-08',\n",
       "  'batch': 0},\n",
       " {'blob': <Blob: 4_data_warehouse, 1_xingtu/douyin_videostat/2024-09-08/douyin_videostat_240918_1.parquet, 1726643490080185>,\n",
       "  'date': '2024-09-08',\n",
       "  'batch': 1},\n",
       " {'blob': <Blob: 4_data_warehouse, 1_xingtu/douyin_videostat/2024-09-11/douyin_videostat_240918_0.parquet, 1726643628548940>,\n",
       "  'date': '2024-09-11',\n",
       "  'batch': 0},\n",
       " {'blob': <Blob: 4_data_warehouse, 1_xingtu/douyin_videostat/2024-09-17/douyin_videostat_240918_0.parquet, 1726643690625356>,\n",
       "  'date': '2024-09-17',\n",
       "  'batch': 0},\n",
       " {'blob': <Blob: 4_data_warehouse, 1_xingtu/douyin_videostat/2024-09-18/douyin_videostat_240918_0.parquet, 1726653162929148>,\n",
       "  'date': '2024-09-18',\n",
       "  'batch': 0},\n",
       " {'blob': <Blob: 4_data_warehouse, 1_xingtu/douyin_videostat/2024-09-19/douyin_videostat_240919_0.parquet, 1726722008069956>,\n",
       "  'date': '2024-09-19',\n",
       "  'batch': 0},\n",
       " {'blob': <Blob: 4_data_warehouse, 1_xingtu/douyin_videostat/2024-10-09/douyin_videostat_241009_0.parquet, 1728459973841830>,\n",
       "  'date': '2024-10-09',\n",
       "  'batch': 0},\n",
       " {'blob': <Blob: 4_data_warehouse, 1_xingtu/douyin_videostat/2024-10-10/douyin_videostat_241010_0.parquet, 1728554975014521>,\n",
       "  'date': '2024-10-10',\n",
       "  'batch': 0},\n",
       " {'blob': <Blob: 4_data_warehouse, 1_xingtu/douyin_videostat/2024-10-11/douyin_videostat_241011_0.parquet, 1728636740720092>,\n",
       "  'date': '2024-10-11',\n",
       "  'batch': 0}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_blobs = [\n",
    "{\n",
    "    \"blob\": blob,\n",
    "    \"date\": blob.name.split('/')[2],\n",
    "    \"batch\": int(blob.name.split('/')[-1].replace(\".parquet\", \"\").split(\"_\")[-1]),\n",
    "} for blob in storage_client.list_blobs(bucket_name,prefix=\"1_xingtu/\" + table_name + \"/\") if blob.name.endswith(\".parquet\")]\n",
    "processing_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file_path': 'gs://4_data_warehouse/1_xingtu/douyin_videostat/2024-09-08/douyin_videostat_240918_0.parquet',\n",
       "  'date': '2024-09-08',\n",
       "  'batch': 0},\n",
       " {'file_path': 'gs://4_data_warehouse/1_xingtu/douyin_videostat/2024-09-08/douyin_videostat_240918_1.parquet',\n",
       "  'date': '2024-09-08',\n",
       "  'batch': 1},\n",
       " {'file_path': 'gs://4_data_warehouse/1_xingtu/douyin_videostat/2024-09-11/douyin_videostat_240918_0.parquet',\n",
       "  'date': '2024-09-11',\n",
       "  'batch': 0},\n",
       " {'file_path': 'gs://4_data_warehouse/1_xingtu/douyin_videostat/2024-09-17/douyin_videostat_240918_0.parquet',\n",
       "  'date': '2024-09-17',\n",
       "  'batch': 0},\n",
       " {'file_path': 'gs://4_data_warehouse/1_xingtu/douyin_videostat/2024-09-18/douyin_videostat_240918_0.parquet',\n",
       "  'date': '2024-09-18',\n",
       "  'batch': 0},\n",
       " {'file_path': 'gs://4_data_warehouse/1_xingtu/douyin_videostat/2024-09-19/douyin_videostat_240919_0.parquet',\n",
       "  'date': '2024-09-19',\n",
       "  'batch': 0},\n",
       " {'file_path': 'gs://4_data_warehouse/1_xingtu/douyin_videostat/2024-10-09/douyin_videostat_241009_0.parquet',\n",
       "  'date': '2024-10-09',\n",
       "  'batch': 0},\n",
       " {'file_path': 'gs://4_data_warehouse/1_xingtu/douyin_videostat/2024-10-10/douyin_videostat_241010_0.parquet',\n",
       "  'date': '2024-10-10',\n",
       "  'batch': 0}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "meta_blob = bucket.blob(\"1_xingtu/\" + table_name + \"/meta.json\")\n",
    "if meta_blob.exists():\n",
    "    processed_blobs = json.loads(meta_blob.download_as_string())\n",
    "else:\n",
    "    processed_blobs = []\n",
    "processed_blobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'batch': 0,\n",
      "  'blob': <Blob: 4_data_warehouse, 1_xingtu/douyin_videostat/2024-10-11/douyin_videostat_241011_0.parquet, 1728636740720092>,\n",
      "  'date': '2024-10-11'}]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "to_process = []\n",
    "for processing_blob in processing_blobs:\n",
    "    processing_date = datetime.strptime(processing_blob[\"date\"], \"%Y-%m-%d\")\n",
    "    if processing_date >= datetime(2024, 9, 8):\n",
    "        if processing_blob[\"batch\"] not in [processed_blob[\"batch\"] for processed_blob in processed_blobs if processing_blob[\"date\"] == processed_blob[\"date\"]]:\n",
    "            to_process.append(processing_blob)\n",
    "pprint(to_process)\n",
    "print(len(to_process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in to_process:\n",
    "    engine = create_engine(\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}\".format(**postgres_config))\n",
    "    l_df = pd.read_sql_table(table_name,con=engine)\n",
    "    r_df = pd.read_parquet(\"gs://\" + bucket_name + \"/\" + item[\"blob\"].name)\n",
    "    m_df = pd.read_sql_table(\"douyin_influencer\",con=engine)\n",
    "    r_df.influencer_id = r_df.influencer_id.astype(str)\n",
    "    m_df.user_id = m_df.user_id.astype(str)\n",
    "    df = pd.merge(m_df, r_df, left_on=\"user_id\", right_on=\"influencer_id\", how=\"inner\")\n",
    "    df = df[[\"id\"] + [column for column in r_df.columns if \"influencer_id\" not in column]]\n",
    "    upsert_df = df.rename(columns={\"id\": \"influencer_id\"})\n",
    "    upsert_df = upsert_df.drop_duplicates(subset=[\"influencer_id\", \"name\"])\n",
    "    query = build_upsert_query(upsert_df.columns, table_name, [\"influencer_id\", \"name\"])\n",
    "    value = \", \".join([str(record).replace(\"''\", \"NULL\") for record in upsert_df.fillna(\"\").to_records(index=False)])\n",
    "    query = query % value\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(query))\n",
    "        conn.commit()\n",
    "    processed_blobs.append({\n",
    "        'file_path': \"gs://\" + bucket_name + \"/\" + item[\"blob\"].name,\n",
    "        'date': item[\"date\"],\n",
    "        'batch': item[\"batch\"]\n",
    "    })\n",
    "    meta_blob.upload_from_string(json.dumps(processed_blobs))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
