{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "from datetime import datetime, timezone\n",
    "from google.cloud import storage\n",
    "import gcsfs\n",
    "import re\n",
    "from sqlalchemy import create_engine, text\n",
    "from typing import List\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgres_config = {\n",
    "    \"host\": \"findy-medium-stage.czmgcqkw4ett.ap-southeast-1.rds.amazonaws.com\",\n",
    "    \"database\": \"findy_medium_stage\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"F!nDy!Med!umStage2o24\",\n",
    "    \"port\": \"5432\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_upsert_query(cols: List[str],\n",
    "                       table_name: str,\n",
    "                       unique_key: List[str]=[],\n",
    "                       cols_not_for_update: List[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Builds postgres upsert query using input arguments.\n",
    "    Note: In the absence of unique_key, this will be just an insert query.\n",
    "    Example : build_upsert_query(\n",
    "        ['col1', 'col2', 'col3', 'col4'],\n",
    "        \"my_table\",\n",
    "        ['col1'],\n",
    "        ['col2']\n",
    "    ) ->\n",
    "    INSERT INTO my_table (col1, col2, col3, col4) VALUES %s\n",
    "    ON CONFLICT (col1) DO UPDATE SET (col3, col4) = (EXCLUDED.col3, EXCLUDED.col4) ;\n",
    "    :param cols: the postgres table columns required in the\n",
    "        insert part of the query.\n",
    "    :param table_name: the postgres table name.\n",
    "    :param unique_key: unique_key of the postgres table for checking\n",
    "        unique constraint violations.\n",
    "    :param cols_not_for_update: columns in cols which are not required in\n",
    "        the update part of upsert query.\n",
    "    :return: Upsert query as per input arguments.\n",
    "    \"\"\"\n",
    "    cols = [f'\"{col}\"' for col in cols]\n",
    "    cols_str = ', '.join(cols)\n",
    "    insert_query = \"\"\" INSERT INTO %s (%s) VALUES %%s \"\"\" % (\n",
    "        table_name, cols_str\n",
    "    )\n",
    "    if cols_not_for_update is not None:\n",
    "        cols_not_for_update.extend(unique_key)\n",
    "    else:\n",
    "        cols_not_for_update = [col for col in unique_key]\n",
    "    cols_not_for_update = [f'\"{col}\"' for col in cols_not_for_update]\n",
    "    unique_key = [f'\"{col}\"' for col in unique_key]\n",
    "    unique_key_str = ', '.join(unique_key)\n",
    "\n",
    "    update_cols = [f\"{col}\" for col in cols if col not in cols_not_for_update]\n",
    "    update_cols_str = ', '.join(update_cols)\n",
    "    update_cols_with_excluded_markers = [f'EXCLUDED.{col}' for col in update_cols]\n",
    "    update_cols_with_excluded_markers_str = ', '.join(\n",
    "        update_cols_with_excluded_markers\n",
    "    )\n",
    "    if len(update_cols) > 1:\n",
    "        equality_clause = \"(%s) = (%s)\"\n",
    "    else:\n",
    "        equality_clause = \"%s = %s\"\n",
    "\n",
    "    on_conflict_clause = f\"\"\" ON CONFLICT (%s) DO UPDATE SET {equality_clause} ;\"\"\"\n",
    "    on_conflict_clause = on_conflict_clause % (\n",
    "        unique_key_str,\n",
    "        update_cols_str,\n",
    "        update_cols_with_excluded_markers_str\n",
    "    )\n",
    "    if len(unique_key) == 0:\n",
    "        return insert_query\n",
    "    return insert_query + on_conflict_clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()\n",
    "today = datetime.now()\n",
    "table_name = \"douyin_video\"\n",
    "api_name = \"search_for_star_hot_item\"\n",
    "bucket_name = \"3_staging_area\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'blob': <Blob: 3_staging_area, 1_xingtu/douyin_video/2024-09-30/search_for_star_hot_item_241001_0.parquet, 1727772493257816>,\n",
       "  'date': '2024-09-30',\n",
       "  'batch': 0},\n",
       " {'blob': <Blob: 3_staging_area, 1_xingtu/douyin_video/2024-10-02/search_for_star_hot_item_241002_0.parquet, 1727845663822959>,\n",
       "  'date': '2024-10-02',\n",
       "  'batch': 0},\n",
       " {'blob': <Blob: 3_staging_area, 1_xingtu/douyin_video/2024-10-06/search_for_star_hot_item_241007_0.parquet, 1728275199308829>,\n",
       "  'date': '2024-10-06',\n",
       "  'batch': 0}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_blobs = [\n",
    "{\n",
    "    \"blob\": blob,\n",
    "    \"date\": blob.name.split('/')[2],\n",
    "    \"batch\": int(blob.name.split('/')[-1].replace(\".parquet\", \"\").split(\"_\")[-1]),\n",
    "} for blob in storage_client.list_blobs(\"3_staging_area\",prefix=\"1_xingtu/douyin_video/\") if api_name in blob.name]\n",
    "processing_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file_path': 'gs://3_staging_area/1_xingtu/douyin_video/2024-09-30/search_for_star_hot_item_241001_0.parquet',\n",
       "  'date': '2024-09-30',\n",
       "  'batch': 0},\n",
       " {'file_path': 'gs://3_staging_area/1_xingtu/douyin_video/2024-10-02/search_for_star_hot_item_241002_0.parquet',\n",
       "  'date': '2024-10-02',\n",
       "  'batch': 0}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "meta_blob = bucket.blob(\"1_xingtu/douyin_video/meta.json\")\n",
    "if meta_blob.exists():\n",
    "    processed_blobs = json.loads(meta_blob.download_as_string())\n",
    "else:\n",
    "    processed_blobs = []\n",
    "processed_blobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'batch': 0,\n",
      "  'blob': <Blob: 3_staging_area, 1_xingtu/douyin_video/2024-10-06/search_for_star_hot_item_241007_0.parquet, 1728275199308829>,\n",
      "  'date': '2024-10-06'}]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "to_process = []\n",
    "for processing_blob in processing_blobs:\n",
    "    if processing_blob[\"batch\"] not in [processed_blob[\"batch\"] for processed_blob in processed_blobs if api_name in processed_blob[\"file_path\"] and processing_blob[\"date\"] == processed_blob[\"date\"]]:\n",
    "        to_process.append(processing_blob)\n",
    "pprint(to_process)\n",
    "print(len(to_process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bracket_duplicated(value):\n",
    "    if len(value) > 6:\n",
    "        return str(value).replace(\"{'{\", \"{\").replace(\"}'}\", \"}\")\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/35cpxwfx6134b2y90f_1cr0m0000gn/T/ipykernel_35027/3089951594.py:22: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '{1, 3}' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  upsert_df.loc[upsert_df[\"item_id\"] == item_id, \"xt_ranking_type\"] = xt_ranking_type\n"
     ]
    }
   ],
   "source": [
    "for item in to_process:\n",
    "    engine = create_engine(\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}\".format(**postgres_config))\n",
    "    l_df = pd.read_sql_table(\"douyin_video\",con=engine)\n",
    "    m_df = pd.read_sql_table(\"douyin_influencer\",con=engine)\n",
    "    r_df = pd.read_parquet(\"gs://\" + bucket_name + \"/\" + item[\"blob\"].name)\n",
    "    r_df[\"influencer_id\"] = r_df[\"influencer_id\"].astype(str)\n",
    "    df = pd.merge(m_df, r_df, left_on=\"user_id\", right_on=\"influencer_id\", how=\"inner\")\n",
    "    df.columns = [column.replace(\"_y\", \"\") for column in df.columns]\n",
    "    df = df[[\"id\"] + list(r_df.columns)]\n",
    "    upsert_df = df.drop(\"influencer_id\", axis=1)\n",
    "    upsert_df = upsert_df.rename(columns={\"id\": \"influencer_id\"})\n",
    "    upsert_df[\"sale_amount\"] = upsert_df[\"sale_amount\"].astype(float).astype(int)\n",
    "    upsert_df[\"comment\"] = 0\n",
    "    upsert_df[\"share\"] = 0\n",
    "    upsert_df[\"create_time\"] = str(today)\n",
    "    upsert_df[\"is_brand\"] = False\n",
    "    upsert_df[\"duration\"] = 0\n",
    "    upsert_df[\"updated\"] = str(today)\n",
    "    upsert_df[\"title\"] = upsert_df[\"title\"].str.replace(\"'\", \"''\")\n",
    "    for item_id in upsert_df[upsert_df[\"item_id\"].duplicated()][\"item_id\"]:\n",
    "        xt_ranking_type = str({value for value in upsert_df[upsert_df[\"item_id\"] == item_id][\"xt_ranking_type\"]})\n",
    "        upsert_df.loc[upsert_df[\"item_id\"] == item_id, \"xt_ranking_type\"] = xt_ranking_type\n",
    "    upsert_df.loc[~upsert_df[\"item_id\"].duplicated(keep=False), [\"xt_ranking_type\"]] = \"{\" + upsert_df[~upsert_df[\"item_id\"].duplicated(keep=False)][\"xt_ranking_type\"].astype(str) + \"}\"\n",
    "    upsert_df[~upsert_df[\"item_id\"].duplicated(keep=False)]\n",
    "    upsert_df[\"xt_ranking_type\"] = upsert_df[\"xt_ranking_type\"].apply(remove_bracket_duplicated)\n",
    "    upsert_df = upsert_df.drop_duplicates()\n",
    "    upsert_df = upsert_df.drop_duplicates(subset=\"item_id\")\n",
    "    query = build_upsert_query(upsert_df.columns, \"douyin_video\", [\"item_id\"])\n",
    "    value = \", \".join([str(record).replace('\"', \"'\") for record in upsert_df.to_records(index=False)])\n",
    "    query = query % value\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(query))\n",
    "        conn.commit()\n",
    "    processed_blobs.append({\n",
    "        'file_path': \"gs://\" + bucket_name + \"/\" + item[\"blob\"].name,\n",
    "        'date': item[\"date\"],\n",
    "        'batch': item[\"batch\"]\n",
    "    })\n",
    "    meta_blob.upload_from_string(json.dumps(processed_blobs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
